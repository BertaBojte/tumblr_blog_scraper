{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by_dict(map, text):\n",
    "    \"\"\"\n",
    "    Replace occurrences of keys in the text with their corresponding values from a dictionary.\n",
    "\n",
    "    This function iterates through each key-value pair in the provided dictionary and \n",
    "    replaces all occurrences of the key in the given text with the corresponding value.\n",
    "\n",
    "    Parameters:\n",
    "        map (dict): A dictionary where keys are the substrings to be replaced and values are the substrings to replace with.\n",
    "        text (str): The text in which to perform the replacements.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified text with all specified replacements made.\n",
    "    \"\"\"\n",
    "    for key, value in map.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_text(url):\n",
    "    \"\"\"\n",
    "    Retrieve the main text content from a blog post.\n",
    "\n",
    "    This function sends a GET request to the specified URL, parses the HTML content using\n",
    "    BeautifulSoup, and extracts the text from all paragraph elements (<p>) that do not\n",
    "    have any specific class attribute. The extracted text is then concatenated into a\n",
    "    single string with each paragraph separated by a newline character.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the blog post to retrieve the text from.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated text content of the blog post.\n",
    "    \"\"\"\n",
    "    \n",
    "    req1 = requests.get(url)\n",
    "    bs1 = BeautifulSoup(req1.content)\n",
    "    pars = bs1.find_all('p', class_ = '')\n",
    "    pars_t = [i.text for i in pars]\n",
    "    full_text = '\\n'.join(pars_t)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_and_tags(text):\n",
    "    \"\"\"\n",
    "    Extract the date and tags from a given text string.\n",
    "\n",
    "    This function processes a text string to extract the date and tags. The text is expected\n",
    "    to contain a date followed by a note count, and then tags prefixed by '#'. The date is\n",
    "    converted to a `datetime.date` object, and the tags are extracted as a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text string containing the date, note count, and tags.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - date (datetime.date): The extracted date.\n",
    "            - tags (list of str): A list of extracted tags.\n",
    "    \"\"\"\n",
    "    date, tags = re.sub('[1]{1} note|\\d notes', '|', text).split('|')\n",
    "    date = pd.to_datetime(date).date()\n",
    "    tags = tags.split('#')\n",
    "    tags = [i for i in tags if len(i)>0]\n",
    "    return date, tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(url, letter_dict):\n",
    "    \"\"\"\n",
    "    Download images from a given URL and save them locally.\n",
    "\n",
    "    This function fetches the HTML content from the specified URL, extracts image sources (excluding\n",
    "    certain images based on their 'alt' attribute), and downloads the images. The images are saved\n",
    "    locally with filenames generated from the URL's title, modified by a dictionary of replacements.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL from which to download images.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs of the downloaded images.\n",
    "    \"\"\"\n",
    "    title = replace_by_dict(letter_dict,url.split('/')[-1])\n",
    "    req1 = requests.get(url)\n",
    "    bs1 = BeautifulSoup(req1.content)\n",
    "    imgs_raw = bs1.find_all('img')\n",
    "    imgs_raw = [i for i in imgs_raw if (i['alt'] not in ['Avatar', ''])]\n",
    "    imgs_good = [i['srcset'].split(',')[-1].split(' ')[1] for i in imgs_raw]\n",
    "    for i,v in enumerate(imgs_good):\n",
    "        im = requests.get(v)\n",
    "        img_data = im.content\n",
    "        with open(f'{title}_{i}.jpg', 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "        \n",
    "    return imgs_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_dict = {'%C3%A1': 'á',\n",
    "               '%C3%A9': 'é',\n",
    "               '%C3%AD': 'í',\n",
    "               '%C3%B3': 'ó',\n",
    "               '%C3%B6': 'ö',\n",
    "               '%C5%91': 'ő',\n",
    "               '%C3%BA': 'ú',\n",
    "               '%C3%BC': 'ü',\n",
    "               \n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the downloaded tumblr archive html file and convert to Beautiful object \n",
    "\n",
    "For the code to work you need to download the archive view of your tumblr and paste the path to the downloaded html file\n",
    "- the download is needed so it contains all the posts\n",
    "- your archive view url should look like something like this: \n",
    "    - https://username.tumblr.com/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the downloaded tumblr archive html file and convert to Beautiful object \n",
    "# for the code to work you need to download the archive view of your tumblr and paste the path to the downloaded html file\n",
    "# the download is needed so it contains all the posts\n",
    "# your archive view url should look like something like this: \n",
    "# https://username.tumblr.com/archive\n",
    "tumblr_archive_html = ''\n",
    "\n",
    "with open(tumblr_archive_html, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "bs = BeautifulSoup(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract all the links of the specific class\n",
    "- Extract the URLs from the links\n",
    "- Extract and convert the titles from the urls (conversion is needed so it is using Hungarian letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_dict = {'%C3%A1': 'á',\n",
    "               '%C3%A9': 'é',\n",
    "               '%C3%AD': 'í',\n",
    "               '%C3%B3': 'ó',\n",
    "               '%C3%B6': 'ö',\n",
    "               '%C5%91': 'ő',\n",
    "               '%C3%BA': 'ú',\n",
    "               '%C3%BC': 'ü',\n",
    "               \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the links of the specific class\n",
    "# Extract the URLs from the links\n",
    "# Extract and convert the titles from the urls (conversion is needed so it is using Hungarian letters)\n",
    "all_links = bs.find_all('a', class_ = 'oKaff QmZ0e')\n",
    "all_urls = [i['href'] for i in all_links]\n",
    "all_titles = [replace_by_dict(letter_dict,i.split('/')[-1]) for i in all_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the functions to scrape all data\n",
    "- text\n",
    "- dates\n",
    "- tags\n",
    "- download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the functions to scrape all data\n",
    "#   - text\n",
    "#   - dates\n",
    "#   - tags\n",
    "#   - download images\n",
    "all_text = [get_blog_text(i) for i in all_urls]\n",
    "print('all text done')\n",
    "all_dates = [get_date_and_tags(i.text)[0] for i in all_links]\n",
    "print('all dates done')\n",
    "all_tags = [get_date_and_tags(i.text)[1] for i in all_links]\n",
    "print('all tags done')\n",
    "[get_images(i, letter_dict) for i in all_urls]\n",
    "print('all images done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the collected data to a pandas dataframe and write it to a csv and a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the collected data to a pandas dataframe and write it to a csv and a json file\n",
    "data = pd.DataFrame({'title': all_titles,\n",
    "                     'urls': all_urls,\n",
    "                     'text': all_text,\n",
    "                     'date': all_dates,\n",
    "                     'tags': all_tags})\n",
    "data.to_csv('erasmus_blog_content.csv', index = False)\n",
    "data.to_json('erasmus_blog_content.json', index=False)\n",
    "data.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
